{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developer: Angel Chiu / @9ukei\n",
    "# Program Name: air_ticket_scraping.ipynb\n",
    "# Date: 2023/08/18\n",
    "# Function: 1. Scraping Air Ticket Prices\n",
    "#           2. Saving Air Ticket Price Data as CSV\n",
    "#           3. Uploading Air Ticket Price Data to Google Sheets\n",
    "#           4. Regular Notifications via Line Notify: Top 3 Airlines with Lowest Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. First, ensure that the `selenium`, `BeautifulSoup4`, and `lxml` libraries are installed on your device.\n",
    "2. If you have already installed these required packages for your project, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install selenium, BeautifulSoup4, lxml\n",
    "# !pip install selenium\n",
    "# !pip install BeautifulSoup4\n",
    "# !pip install lxml\n",
    "# !pip install pyshorteners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/your_path/your_project/driver/chromedriver')\n",
    "\n",
    "# Crawl the URL that you wanted to scrape on Line travel\n",
    "# Here, I will take the example of comparing flight prices for flights from Taipei (TPE) to Osaka (OSA) for the dates 9/11 to 9/16.\n",
    "air_ticket_url = '''\n",
    "https://travel.line.me/flights/list?roundType=1&cabinClass=1&numOfAdult=2&numOfChildren=0&numOfBaby=0&linePointsRebateOnly=1&departureAirports=&departureCities=TPE&departureDates=1694390400000&arrivalAirports=&arrivalCities=OSA&departureAirports=&departureCities=OSA&departureDates=1694822400000&arrivalAirports=&arrivalCities=TPE\n",
    "'''\n",
    "\n",
    "# This number of seconds can be adjusted and increased according to the network delay problem. \n",
    "# It is recommended to stay at least 10 seconds or more.\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(air_ticket_url)\n",
    "\n",
    "# load more results to maximize the scraping\n",
    "def page_scrolldown():\n",
    "    try:\n",
    "        for i in range(1,20):\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "            time.sleep(0.7)\n",
    "    except:\n",
    "        print('Check to see if any code is causing the error.')\n",
    "        pass\n",
    "page_scrolldown()\n",
    "\n",
    "# view the html source code of the website\n",
    "driver.page_source\n",
    "\n",
    "html_source_code = driver.page_source\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_source_code, 'html.parser')\n",
    "\n",
    "# check source code without tags\n",
    "# print(soup.text)\n",
    "\n",
    "# Display the total number of tickets found\n",
    "record = soup.select_one('#__next > div.css-1cp3u8n.e1a1bycy0 > div:nth-child(2) > span')\n",
    "print('æœå°‹çš„æ©Ÿç¥¨æ¯”åƒ¹ç¸½ç­†æ•¸ï¼š',record.text)\n",
    "#\n",
    "\n",
    "time_loc_list = []\n",
    "\n",
    "for time_loc_info in soup.select('#__next > div.css-1cp3u8n.e1a1bycy0 > div:nth-child(2)'):\n",
    "    for tl in time_loc_info.select('.css-nkthol.eooboqb1, .css-1qzlsgj.e4pxchi2'):\n",
    "        time = tl.text\n",
    "        time_loc_list.append(time)\n",
    "\n",
    "# Iterate through the elements in the time_loc_list, taking eight elements at a time to form a tuple, \n",
    "# and place these tuples into the tuple_list.\n",
    "tuple_list = [tuple(time_loc_list[i:i+8]) for i in range(0, len(time_loc_list), 8)]\n",
    "# print(tuple_list)\n",
    "\n",
    "df_time = pd.DataFrame(tuple_list, columns=[\"èµ·é£›æ™‚é–“(å‡ºç™¼)\",\"èµ·é£›åœ°é»(å‡ºç™¼)\",\"æŠµé”æ™‚é–“(å‡ºç™¼)\",\"æŠµé”åœ°é»(å‡ºç™¼)\", \"èµ·é£›æ™‚é–“(å›ç¨‹)\",\"èµ·é£›åœ°é»(å›ç¨‹)\", \"æŠµé”æ™‚é–“(å›ç¨‹)\",\"æŠµé”åœ°é»(å›ç¨‹)\"])\n",
    "\n",
    "result = []\n",
    "\n",
    "# Depart/Arrived time + location (aboard/arrived)\n",
    "for time_loc_info in soup.select('#__next > div.css-1cp3u8n.e1a1bycy0 > div:nth-child(2)'):\n",
    "    for tl in time_loc_info.select('.css-1eowobi .css-j7qwjs'):\n",
    "        time_loc = tl.text\n",
    "        # print(time_loc)\n",
    "\n",
    "air_ticket_info = soup.find_all(class_='css-1eowobi')\n",
    "\n",
    "for ticket_info in air_ticket_info:\n",
    "    # airline company\n",
    "    airline = ticket_info.find(class_='css-84a4s3 e1fe20ih3').getText().strip()\n",
    "    # air ticket provider\n",
    "    ticket_site = ticket_info.find(class_='css-6x2xcr e1fe20ih2').getText().strip()   \n",
    "    # ticket per price\n",
    "    ticket_per_price = ticket_info.find(class_='css-iw7h7v ejxn77z0').getText().strip().replace(',', '')\n",
    "    # total ticket price(2 ppl)\n",
    "    total_price = ticket_info.find(class_='css-wycfi3 e1fe20ih3').getText().strip('')[9:].replace(',', '')\n",
    "    # ticket purchase url\n",
    "    ticket_purchase_url = 'https://travel.line.me/' + ticket_info.find('a').get('href')\n",
    "\n",
    "    # Using TinyURL Short URL\n",
    "    import pyshorteners\n",
    "\n",
    "    s = pyshorteners.Shortener()\n",
    "    ticket_purchase_short_url = s.tinyurl.short(ticket_purchase_url)\n",
    "\n",
    "    result.append((airline,ticket_site,int(ticket_per_price),int(total_price), ticket_purchase_short_url))\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"èˆªç©ºå…¬å¸\", \"è³¼è²·ç¶²ç«™\", \"ä¸€äººåƒ¹æ ¼(TWD)\", \"å…©äººç¸½åƒ¹(TWD)\", \"è²·ç¥¨å»ï¼\"])\n",
    "\n",
    "# print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If df1 and df2 have the same number of rows but different column names, you can use the concat function to merge them together. \n",
    "# The concat function allows you to concatenate multiple DataFrames along a specified axis. \n",
    "# In this case, you can choose to concatenate along the row axis.\n",
    "\n",
    "ticket_full_info = pd.concat([df_time, df], axis=1)\n",
    "ticket_full_info.align\n",
    "ticket_full_info.index +=1\n",
    "print(ticket_full_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the scraped file to local and store it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "\n",
    "month_str = f\"{now.month:02d}\"\n",
    "day_str = f\"{now.day:02d}\"\n",
    "date = f\"{now.year}{month_str}{day_str}\"\n",
    "\n",
    "# Current time\n",
    "loc_dt = datetime.datetime.today() \n",
    "loc_dt_format = loc_dt.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "# dataframe to csv\n",
    "scapring_date = date\n",
    "ticket_full_info.to_csv(f\"{scapring_date}_air_ticket_full_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync data to Google Sheets by Pygsheets\n",
    "- To sync the air ticket data to `Google Sheets` in real time, we can use a Python module called `Pygsheets` to control Google Sheet API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygsheets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsheets\n",
    "\n",
    "auth_file = \"credentials.json\"\n",
    "gc = pygsheets.authorize(service_file = auth_file)\n",
    "\n",
    "# sheet read by pygsheets\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/yoursheeturlnamexxxxx\" \n",
    "sheet = gc.open_by_url(sheet_url)\n",
    "\n",
    "# Select by name\n",
    "air_ticket_sheet_01 = sheet.worksheet_by_title(\"air ticket price comparison\")\n",
    "\n",
    "# Update values in the worksheet\n",
    "# title_date = 'A1'\n",
    "# air_ticket_sheet_01.update_values(title_date, [['å°ç£å¤§é˜ªä¾†å›æ©Ÿç¥¨å³æ™‚æ¯”åƒ¹å ±è¡¨' + '\\n' + loc_dt_format]])\n",
    "attributes = 'A1'\n",
    "air_ticket_sheet_01.update_values(attributes, [[\"èµ·é£›æ™‚é–“(å‡ºç™¼)\",\"èµ·é£›åœ°é»(å‡ºç™¼)\",\"æŠµé”æ™‚é–“(å‡ºç™¼)\",\"æŠµé”åœ°é»(å‡ºç™¼)\", \"èµ·é£›æ™‚é–“(å›ç¨‹)\",\"èµ·é£›åœ°é»(å›ç¨‹)\", \"æŠµé”æ™‚é–“(å›ç¨‹)\",\"æŠµé”åœ°é»(å›ç¨‹)\",\"èˆªç©ºå…¬å¸\", \"è³¼è²·ç¶²ç«™\", \"ä¸€äººåƒ¹æ ¼(TWD)\", \"å…©äººç¸½åƒ¹(TWD)\", \"è²·ç¥¨å»ï¼\"]])\n",
    "start_record = 'A2'\n",
    "# `df.values.tolist()` method can transform the data type `dataframe` to `list`\n",
    "air_ticket_sheet_01.update_values(start_record, ticket_full_info.values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Notify ä¸²æ¥ + Cron Job\n",
    "- Use `Line Notify` to notify the prices of the three cheapest flight tickets obtained from the web scraping.\n",
    "- Send the prices of the three cheapest flight tickets at a fixed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def send_notification():\n",
    "\n",
    "    # Read the data from Google Sheets\n",
    "    data = air_ticket_sheet_01.get_all_records()\n",
    "\n",
    "    # Extract the top three combinations of airlines with the lowest prices\n",
    "    top_three_rows = data[:3]\n",
    "    \n",
    "    # Send LINE Notify notification   \n",
    "    line_notify_url = \"https://notify-api.line.me/api/notify\"\n",
    "\n",
    "    msg = '\\nå°ç£å¤§é˜ªä¾†å›æ©Ÿç¥¨\\nå³æ™‚æ¯”åƒ¹å ±è¡¨\\n\\n'\n",
    "    for row in top_three_rows:\n",
    "        msg += f'''ğŸ’èˆªç©ºå…¬å¸: {row[\"èˆªç©ºå…¬å¸\"]}\\nğŸ’ä¸€äººåƒ¹æ ¼(TWD): {row[\"ä¸€äººåƒ¹æ ¼(TWD)\"]}å…ƒ\\nğŸ’å…©äººç¸½åƒ¹(TWD): {row[\"å…©äººç¸½åƒ¹(TWD)\"]}å…ƒ\\nâœˆï¸å“ªæ¬¡ä¸è¡äº†ï¼Œè²·ç¥¨å»ï¼\\n{row[\"è²·ç¥¨å»ï¼\"]}\\n\\n'''\n",
    "    \n",
    "    payload={'message':{msg}}\n",
    "    headers = {'Authorization': 'Bearer ' + 'yourtokenhere'}\n",
    "    # Status code\n",
    "    response = requests.request(\"POST\", line_notify_url, headers=headers, data=payload)\n",
    "    print(response.text)\n",
    "\n",
    "# Set the time for sending notifications (Example: Every day at 10 AM)\n",
    "schedule.every().day.at(\"10:00\").do(send_notification)\n",
    "\n",
    "# Infinite loop to keep the script running\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
